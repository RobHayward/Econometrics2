\documentclass[12pt, a4paper, oneside]{article} % Paper size, default font size and one-sided paper
%\graphicspath{{./Figures/}} % Specifies the directory where pictures are stored
%\usepackage[dcucite]{harvard}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{pdflscape}
\usepackage{rotating}
\usepackage[flushleft]{threeparttable}
\usepackage{multirow}
\usepackage[comma, sort&compress]{natbib}% Use the natbib reference package - read up on this to edit the reference style; if you want text (e.g. Smith et al., 2012) for the in-text references (instead of numbers), remove 'numbers' 
\usepackage{graphicx}
%\bibliographystyle{plainnat}
\bibliographystyle{agsm}
\usepackage[colorlinks = true, citecolor = blue, linkcolor = blue]{hyperref}
%\hypersetup{urlcolor=blue, colorlinks=true} % Colors hyperlinks in blue - change to black if annoying
%\renewcommand[\harvardurl]{URL: \url}
\usepackage{listings}
\usepackage{color}
\definecolor{mygrey}{gray}{0.95}
\lstset{backgroundcolor=\color{mygrey}}
\begin{document}
\title{Econometrics}
\author{Rob Hayward}
\date{\today}
\maketitle
\section{Panel data}
This will come from the vignette \href{http://cran.r-project.org/web/packages/plm/vignettes/plm.pdf}{The plm package} available on CRAN. This is also called \emph{longitudinal data} in other fields. 

The general model
\begin{equation}
y_{it} = \alpha_{it} + \beta_{it}^Tx_{it} + u_{it}
\end{equation}
where, $i = 1 \dots n$ is the individual (group or country), $t = 1 \dots T$ is time and $u$ is a random disturbance with mean zero. 

A number of assumptions are required to estimate the model. A common assumption is that $\alpha_{it} = \alpha$ for all i and t and $\beta_{it} = \beta$ for all i and t. This gives the \emph{pooling model}
\begin{equation}
y_{it} = \alpha = \beta^T X_{it} + u_{it}
\end{equation}

To model individual hetrogeneity, it is possible to assume that the error term is composed of two parts, one of which is specific to the individual that does not change over time. This is the \emph{unobserved effects} model. 

\begin{equation}
y_{it} = \alpha + \beta^T x_{it} + u_i + \varepsilon_{it}
\end{equation}

The estimation method depends on the proprerties of the two error components. Either of these can be assumed to be independnt of the regressors and the other error term. It is usual to assume that the ideosyncratoc error is independent of each.  If the individual error is correlated with the regressors, the OLS estimate of $\beta$ would be inconsistent so it is usual in this case to treat $u_i$ as another set of parameters to be estimated. This means that $\alpha_i = \alpha_{it}$.  This is called the \emph{fixed effects} model. 

If it is assumed that the individual component $u_{it}$ is uncorrelated with the regressors, a \emph{random effects} model may be computed. OLS may be consistent but correlation across the composite error term means that feasible general least squares estimation is required. A \emph{first difference} estimator can be used if there is serial correlation in the errors.  The \emph{between} model is computed on the average individual values over time.  It discards information about intragroup variability. 

The usual method is to 
\begin{itemize}
\item Test for poolability (do the coefficients apply over all individuals?)
\item If coefficients appear to be stable, look for group effects in individuals and time
\item Use Hausman-type test to establish fixed or random effects if hetrogeneity is established. Use random effects if possible as it is more robust. 
\item Test the error term one effects are established. 
\end{itemize}

There are also problems with the dynmamic elements and the faiure of strict exogeneity. 

\subsection{Data structure}
\lstinline{pdata.frame} function will create the panel version of a data.frame.  It is assumed that the first two columns are the individal and time indices or elese the appropriate columns are identified. 

\lstinline{plm} function will estimate with different models and different effects. 

<<data, warning=FALSE, message=FALSE>>=
library(plm)
data("Grunfeld")
head(Grunfeld)
@
<<data2, warning=FALSE, message=FALSE>>=
data("EmplUK")
E <- pdata.frame(EmplUK, index = c("firm", "year"), drop.index = TRUE,
                  row.names = TRUE)
head(E)
@

There are particular methods for the \lstinline{pseries} that come from extracting a series from a \lstinline{pdata.frame}. \lstinline{summary}will compare the variance of the variable that is due to individual and time components; the \lstinline{matrix} will create a matrix with individual as rows and time as columns.  

<<data3, message=FALSE, warning=FALSE>>=
summary(E$emp)
head(as.matrix(E$emp))
@
There are \lstinline{Between, between} and \lstinline{Within} functions to compute the mean and the individual deviation from the mean. 
<<data4, message=FALSE, warning=FALSE>>=
head(lag(E$emp, 0:2))
@
\subsection{Estimation}
Several different models can be estimated with the standard \lstinline{plm} method. 
<<estimate, warning=FALSE, message=FALSE>>=
grun.fe <- plm(inv ~ value + capital, data = Grunfeld, model = "within")
grun.re <- plm(inv ~ value + capital, data = Grunfeld, model = "random")
summary(grun.re)
@
The fixed effects can be extracted using the \lstinline{fixef} function, specifying whether this is measured as the level, the deviation from the mean or the deviation from the first level. 
<<fixed, warning=FALSE, message=FALSE>>=
summary(fixef(grun.fe, type = "dmean"))
@



\section*{Asymptotic theory}
As an overview of asymptotic theory.  \href{http://davegiles.blogspot.com/2014/10/illustrating-asymptotic-behaviour-part-i.html}{Dave Giles} uses a monte carlo experiment to illustrate the behaviour of the OLS estimator.  

\begin{equation*}
y_t = \beta_0 + \beta_1 y_{t-1} + \varepsilon_t, \quad t = 2, 3 \dots \quad y_1 = 0
\end{equation*}

$\varepsilon$ is generated according to a \emph{uniform} distrubution on the interval $(-1, +1)$.

The attention here is on $\beta_1$.  As the estimator is a sample statistic (a function of random sample data) this disrtibution is the \emph{sampling distribution}.  The estimate may change as $n$ changes. The form of the distribution also depends on the size of the sample. 

<<sample, fig.height=4, cache=TRUE>>=
Asymp <- function(n, reps){
y <- rep(NA, n)
y[1] <- 0
temp <- rep(NA, reps)
for(j in 1:reps){
for(i in 2:n){
  e <- runif(n, -1, 1)
  y[i] <- 1 + 0.5*y[i-1] + e[i]
  
}
ylag <- lag(y)
da <- data.frame(y[2:n], y[1:n-1])
colnames(da) <- c("y", "ylag")
eq <- lm(y ~ ylag, data = da)
temp[j] <- eq$coefficients[2]
}
hist(temp)
mean(temp)
}
Asymp(20, 50)
@
The estiamte of the coefficient of the lagged dependent variable is biased downwards.  There is also a negative skew to the estimate of the coefficient on the dependent varialbe.  The estimator of $\beta_1$ is \emph{consistent}. This means that as the sample gets bigger, the estimate moves towards the true value. 

There is a \href{http://davegiles.blogspot.com/2014/10/illustrating-asymptotic-behaviour-part_12.html}{part two}. If $x^*$ is the sample mean, this is an unbiased estimator of $\mu$ and the variance of $x^*$ is $\sigma^2/n$.  While the raw variance will disapear asympotitcally, the scalled variance will not. The scalled statistic is $x^*n^{0.5}$.  When focusing on this scalled statistic, notice tht the varince is $(n^{1/2})^2var(x^*) = \sigma^2$.  


\href{http://davegiles.blogspot.com/2014/10/illustrating-asymptotic-behaviour-part.html}{Part three}.

It is possible to compate the OLS estimator with another estimator (say the Least Absolute Deviation (LAD)).  If the scaled ('normalised') sampling distributions are compared to assess relative efficiency.  These are $n{^0.5}(b_2 - \beta_2)$ and $N^{0.5}(b^*_2 - \beta_2)$ respectively, where $b_2$ is the OLS estimator or $\beta_2$ and $b_2^*$ is the LAD estimtor of $\beta_2$.  

The comparison can be made with bias, variance and MSE.  The estimators have different properties when the sample size is small but both are asymptotically unbiased and consistent. However, the variance of the asymptotic distrubtion of the LAD estimator is smaller that the asymptotic distribtion of the OLS estimator. The distribution of the estmators becomes normal after a sample size of about 5000. 

\section*{ARDL models}
This comes from \href{http://davegiles.blogspot.ca/2013/03/ardl-models-part-i.html}{Dave Giles ARDL}.  Once again, thanks Dave. 

The basic form of the model is

\begin{equation}\label{ARDL}
y_t = \beta_0 + \beta_1 y_{t-1} + \dots + \beta_k y_{t-p} +\alpha_0 x_t + \alpha_1 x_{t-1} \dots \alpha_q x_{t-q} + \varepsilon_t
\end{equation}

This is an ARDL(p, q) model.  The model has an \emph{autoregressive element}. As a result of the lagged values of the dependent variable, this model will yield \emph{biased estimates} of the parameters; if the error term has \emph{autocorrelation} the OLS estimates will be \emph{inconsistent}.  

\href{http://ideas.repec.org/p/dgr/eureir/1765001190.html}{Frances and Oest (2004)} provide a historical overview of the \emph{Koyck model}. There is also the \href{http://www.econometricsociety.org/abstract.asp?ref=0012-9682&vid=33&iid=1&aid=0012-9682%28196501%2933%3A1%26lt%3B178%3ATDLBCA%26gt%3B2%2E0%2ECO%3B2-0&s=-9999}{\emph{Almon distributed lag model}}.  \href{http://www.amazon.com/Distributed-Lags-Estimation-Formulation-Textbooks/dp/0444860134/ref=sr_1_sc_2?s=books&ie=UTF8&qid=1362589275&sr=1-2-spell&keywords=dhrymas+distributed+lag}{Dhrymes (1971)} provides an overview. 

\href{http://davegiles.blogspot.ca/2013/06/ardl-models-part-ii-bounds-tests.html}{Second part}.  Thanks Dave. 

This is an implementation of the \emph{Bounds Test}.  This is a test to see if there is a long-run relationship. As usual, there are three types of data that may be encountered: 
\begin{itemize}
\item stationary data that can be modelled in the levels with OLS
\item non-stationary data (say $I(0)$ data) that can be modelled in first difference with OLS
\item non-statonary data that are integrated and cointegrated. These data can give us a long-run relationship with OLS and and a short-term correction with the \emph{Error-correction model}
\end{itemize}

The ARDL/Bounds methodology of Persaran and Shinn (1999) and Perseran et al (2001) has a number of features that make it attractive.

\begin{itemize}
\item It can be used with a mixture of $I(0)$ and $I(1)$ data.  
\item It involves just a single equation set-up
\item Different variables can be assigned different lag lengths.
\end{itemize}

The road map is as follows 
\begin{enumerate}
\item Make sure that none of the variables are $I(2)$
\item formulate an \emph{unrestricted} ECM
\item Determine the appropriate lag structure
\item Make sure that the errors are serial independent
\item Make sure that the model is \emph{dynamicall stable}
\item Perform a \emph{Bounds Test} to see if there is evidence of a long-run relationship
\item If the answer to the previous question is "yes", estimate a long-run model as well as a \emph{restricited} ECM. 
\item Use these results to estimate the long run relationship and the short run relatinship
\end{enumerate}

\subsection*{Step one}
Use the ADF and KPSS tests for $I(2)$

\subsection*{Step two}
Formulate the model

\begin{equation}
\Delta y_1 = \beta_0 + \sum \beta_i \Delta y_{t-i} + \sum \gamma_j \Delta x_{1, t-j} + \sum \delta_k \Delta x_{2, t-k} + \theta_0 y_{t-1} + \theta_1 x_{1, t-1} + \theta_2 x_{2, t-1} + \varepsilon_t
\end{equation}

This is like an unrestricted ECM. 

\subsection*{Step three}
The appropriate lag lengths for $p1$, $q1$ and $q2$ need to be selected.  Zero length lags may not be required.  This is usually carried out with \emph{Information Cirteria}.  Dave uses a combination of SIC and significance of coefficients. 

\subsection*{Step four}
A key assumption is that the errors must be serially independent. This requirement can also influence the selection of lag length. Use the LM test to test the null that there is serial independence against the alternative that they are AR or MA. 

\subsection*{Step five}
The model must be tested for \emph{dynamic stability}.  There is more \href{http://davegiles.blogspot.ca/2013/06/when-is-autoregressive-model.html}{here}.  This essentially means that the auto-regressive coefficients must lie within the unit circle and so there is no \emph{unit root}.  The roots of the \emph{characteristic equation} must lie outside the unit circle. 

\subsection*{Step six}
Now perform the \emph{F-test} of the hypothesis $H0: \theta_0 = \theta_1 = \theta_2 = 0$; against the alternative that $H0$ is not true. As in the conventional \emph{conintegraton test}, this is a test of \emph{absence} of a long-run relationship. The distribution of the F-statistic is non-standard.  However, Pesaran et al have \emph{bounds} on the critical values for the \emph{asymptotic} distribution of the F-test for different number of variables that range from the case of $I(0)$ to $I(1)$.  If the F statistic falls below the lower bound, conclude that the variables are $I(0)$ so there is no cointegration; if the test exceeds the upper bound, there is contegration. 

As a cross-check, test $H0: \theta_0 = 0$ against the alternative $H1: \theta_0 < 0$

\subsection*{Step seven}
If the bounds tests suggestst that there is cointegration, the relationship can be estimated. 

\begin{equation}
y_t = \alpha_0 + \alpha_1 x_{1, t} + \alpha_2 x+{2, t} + \varepsilon_t
\end{equation}
and the ECM

\begin{equation}
\Delta y_t = \beta_0 + sum \beta_i \Delta y_{t-i} + \sum \gamma_j \Delta x_{1, t-j} + \sum \delta_k x_{2, t-k} + \psi z_{t-1} + \varepsilon
\end{equation}

where $z_{t-1} = y_{t-1} - a_0 - a_1 x_{1,t} - a_2 x_{2,t}$

\subsection*{Step Eight}
Extract the long-run effects from the unrestricted ECM.  From Equation \ref{ARDL}, note that in the long-run $\Delta y_t = \Delta x_{1,t} = \Delta x_{2,t} = 0$ and therefore, the long-run coefficients for $X_1$ and $X_2$ are $-(\theta_1/\theta_0)$ and $-(\theta_2/\theta_0)$ respectively. 


\subsection*{Example}
COmplete this after the Granger Causality. 




\section*{Dynamic Stability}
An AR(p) process of the form, 

\begin{equation}
y_t = \gamma_1 y_{t-1} + \gamma_2 y_{t-2} \dots \gamma_p y_{t-p} + \varepsilon
\end{equation}

Will be dynamically stable if the roots of the \emph{characteristic equation} 

\begin{equation}
1 - \gamma_1 z - \gamma_2 z^2 \dots \gamma_p z^p = 0
\end{equation}

lie \emph{strictly outside} the unit circle

or, if the characteristic equation is defined as 

\begin{equation}
z^p - \gamma_1 z^{p-1} - z^{p-2} \dots \gamma_P = 0
\end{equation}

lie \emph{strictly inside} the unit circle.

Therefore, with a AR(1) model, $p = 1$ and the characteristic equation is 

\begin{equation}
1 - \gamma_1 z = 0
\end{equation}

Solving for z, $z = 1/\gamma_1$, so the stationarity condition is that $|1/\gamma_1 | > 1$ or$ |\gamma_1 | >1$.  With an AR(2), 

\begin{equation}
1 - \gamma_1 z - \gamma_2 z^2 =0
\end{equation}

lie strictly \emph{outside} the unit circle, or

\begin{equation}
z^2 - \gamma_1 z - \gamma_2 = 0
\end{equation}

must be strictly \emph{inside} the unit circle. 

\section*{Grange Causality}
\href{http://davegiles.blogspot.ca/2011/04/testing-for-granger-causality.html}{Granger Causality}.  I need to go through this. 

\section{GLM}
This comes from the Coursera Regression Models course.  The video files are in the Teaching-Econometrics-Coursera folders.  Some initial thoughts.

Think about the confidence intervarls round the predicted values.  This is outlined in week 2 and predicted intervals.  I may need an update. 

\subsection*{Interactions}
Using the regular linea model
\begin{equation}
Hu_i = b_0 +b_1Y_i + e_i
\end{equation}

Relationship between hunger and year can be estimated for male and female.  This can be two separate models. This produces two different models. Alteratnvely, it is possible to estimate one model with different intercepts for each. 

\begin{equation}
Hu_i = b_0 +b_11(Sex_i = "Male") + b_2Y_i + e_i
\end{equation}

Intercept is $b_0$ for females and $b_0 + b_2$ for males. 

It is also possible to create a model where there are two different slopes for each model. 

\begin{equation}
Hu_i = b_0 +b_11(Sex_i = "Male") + b_2Y_i + b_31(Sex_i = "Male") \times Y_i + e_i
\end{equation}

Now the intercept for males is $b_0 + b_1$ while it is $b_0$ for females; the slope is $b_2 Y_i + b_3 Y_i$ for males and $b_2$ for females. 

For the interaction term, using $*$ will automatically include the terms on their own, using $:$ will only include the interaction in the model, excluding the component parts. 

This can be extended to continuous variables.  For example, 

\begin{equation}
Hu_i = b_0 +b_1Inc_i + b_2Y_i + b_3Inc_i \times Y-I + e_i
\end{equation}

Now $b_3$ is the interaction or the rate of change of hunger for a change 
in income.

There are a number of exercises that are carried out.  The code for the examples is \href{https://github.com/bcaffo/courses/blob/master/07_RegressionModels/02_03_adjustment/index.Rmd}{here}. There are a huge variety of relationships and also the questions, once this has been done, of causation. 
\subsection*{Residuals}
Outliers have a large potential to affect the results. There are a number of \lstinline{influence.measures} measures. The list includes
\begin{itemize}
\item \lstinline{rstandard}: standardised residuals or residuals divided by the standard deviation. 
\item \lstinline{rstudent}: ith residual is deleted. 
\item \lstinline{dffits}:  measures the influence of residuals on prediction. 
\item \lstinline{cooks.distance}: measures the effect on coefficient when residual is delated. 
\item \lstinline{resid}: returns the ordinary residuals.
\item \lstinline{resid(fit)/(1 - hatvalues(fit))} where \lstinline{fit}
\end{itemize} is the linear model fit.  This returns the PRESS residuals. 

<<resid, fig.height = 4, fig.width=4>>=
n <- 100
x <- c(10, rnorm(n))
y <- c(10, c(rnorm(n)))
plot(x, y, frame = FALSE, cex = 2, pch = 21, bg = "lightblue", 
     col = "black")
abline(lm(y ~ x))
@
The estimated coefficients will change dramatically if points are taken out. 
\subsection*{Model selection}
Difference in assessing models and assessing predictability. Model is a lense through which to look at the data. Different models for prediction, for studying mechanisms and looking at causual effects. 

General rules
\begin{itemize}
\item Omitting variables can result in bias in the coefficients unless their regressors are uncorrelated with the omitted variables. Therefore, randomising trials will attempt to ensure that there is no correlation. 
\item Including unnecessary variables will increase the standard error of the regression variables. 
\end{itemize}

variance inflation is much worse when the extra variables that are added to the model are higherly related to x1.  There are some simulations in the slides that show this. The \emph{variance inflation factor} will show how much the variance of estimate will increase by adding other variables.  The function to use is \lstinline{vif} as in \lstinline{vif(fit); fit <- lm(x ~ y)}.  Nested models can be tested with ANOVA.

\subsection*{GLM}
Limitations of linear models
\begin{itemize}
\item Do not work well for discrete or stricly-positive data
\item Transformations may be difficult to intpret
\item Transformations like logs are not always applicable. 
\end{itemize}

\emph{Generalilzed Linar Models} introduced in a 1972 paper by Nelder and Wedderburn.  There are three components:
\begin{itemize}
\item An exponential family modle for the response
\item A systematic component via the linar predictor
\item A linnk function that connects the means of the response to the linear predictor
\end{itemize}

The GLM model is
\begin{equation}
Y_i \sim N(\mu_i, \sigma^2)
\end{equation}

The Gaussian is an exponential distribution. 
\begin{itemize}
\item The \emph{linear predictor} is $\eta_i = \sum_{k = i}^p X_{ik}\beta_k$.  
\item The \emph{link function} is $g(\mu) = \mu$ so that, for linear models, $\eta_i = \mu_i$
\end{itemize}

This yields the \emph{likelihood model} as an additive Gaussian linear model. 

\begin{equation}
Y_i = \sum_{k = 1}^p X_{ik}\beta_k + \varepsilon_i
\end{equation}

\subsubsection*{Logistic model}
Assume that $Y_i \sim Bernoulli(\mu_i)$ so that $E[Y_i]  = \mu_i$ and where $0 \leq \mu_i \leq 1$. 

The linear predictor is $\eta_i = \sum_{k=1}^p X_{ik}\beta_k$

The link function $g(\mu) = \eta = log(\frac{\mu}{1 - \mu})$.  This is the log-odds fnction.\footnote{$\frac{/mu}{1-\mu}$. The probability divided by one minus the probability is the odds.  Therefore the logit is the log of the odds.}  The logit can be inverted so that $\mu_i = \frac{exp(\eta_i)}{1 + exp(\eta_i)}$ and $1 - \mu_i = \frac{1}{1 + exp(\eta_i)}$. 

Therefore, the likelihood is
\begin{equation}
\prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1-y_i}
= \exp\left(\sum_{i=1}^n y_i \eta_i \right)
\prod_{i=1}^n (1 + \eta_i)^{-1}
\end{equation}

\subsubsection*{Poisson regression}
Assume that $Y_i \sim Poisson(\mu_i$ so that $E[Y_i] = \mu_i$ and $\mu_i > 0$. 

The linear predictor is $\eta_i = \sum_{k=1}^p X_{ik}\beta_k$

Link function $g(\mu) = \eta = log(\mu)$.  Therefore, 

$mu_i = e\eta^i$

Therefore, the likelihood function is, 

\begin{equation}
\prod_{i=1}^n (y_i !)^{-1} \mu_i^{y_i}e^{-\mu_i}
\propto \exp\left(\sum_{i=1}^n y_i \eta_i - \sum_{i=1}^n \mu_i\right)
\end{equation}

These notes are copied directly from the slides. 

In each case, the only way in which the likelihood depends on the data is through 

$$\sum_{i=1}^n y_i \eta_i =
\sum_{i=1}^n y_i\sum_{k=1}^p X_{ik} \beta_k = 
\sum_{k=1}^p \beta_k\sum_{i=1}^n X_{ik} y_i
$$
Thus if we don't need the full data, only $\sum_{i=1}^n X_{ik} y_i$. This simplification is a consequence of chosing so-called 'canonical' link functions.
All models acheive their maximum at the root of the so called normal equations
$$
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{Var(Y_i)}W_i
$$
where $W_i$ are the derivative of the inverse of the link function.

Notes about variances.  For Bernouilli and Poisson cases, there are direct realtionships between the mean and the variance that can be relaxed a little by introducing the variable $\phi$. 
$$
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{Var(Y_i)}W_i
$$
* For the linear model $Var(Y_i) = \sigma^2$ is constant.
* For Bernoulli case $Var(Y_i) = \mu_i (1 - \mu_i)$
* For the Poisson case $Var(Y_i) = \mu_i$. 
* In the latter cases, it is often relevant to have a more flexible variance model, even if it doesn't correspond to an actual likelihood
$$
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i (1 - \mu_i ) } W_i ~~~\mbox{and}~~~
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i} W_i
$$
These are called 'quasi-likelihood' normal equations when using the $\phi$ function.  In $R$ it is possible to use the \lstinline{quasi-poisson} or \lstinline{quasi-binomial} families to get this flexibility. 

The equations have to be solved iteratively. The predicted linear predictor responses can be obtained as $\hat{\eta} = \sum_{k = 1}^p X_b\beta_k$ and the predicted mean response as $\hat{\mu} = g^{-1} (\hat{\eta})$.

Coefficients are interpreted as 
$$
g(E[Y | X_k = x_k + 1, X_{\sim k} = x_{\sim k}]) - g(E[Y | X_k = x_k, X_{\sim k}=x_{\sim k}]) = \beta_k
$$. 

This is the change in the link function of the expected response for unit of change in $X_k$ holding other regressors constant. Uses variation on the Newon/Raphson algorithm. Asypotics are used for inference.

\subsubsection*{GLM binary data}
Two possible outcomes mean that there is binary or Bernoulli data. 

<<RavensdataE>>=
load("../Data/ravensData.rda")
head(ravensData)
@

The results can be built up gradually. 
\begin{itemize}
\item The binary outcome is win or lose $$RW_i$$ 
\item Probability of winning $$\rm{Pr}(RW_i | RS_i, b_0, b_1 )$$
\item Odds of winning $$\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}$$ 
\item Log odds $$\log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right)$$ 
\end{itemize}

Interpreting the results.  From
$$\log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right)$$
$b_0$ is the log odds of winning if there are no point scored.  $b_1$ is the log odds of a win for each point scored (leaving all other variables constant).  $exp(b_1)$ is the odds ratio of a win for each point scored (holding all else constant). 

\subsubsection*{Odds}
- Imagine that you are playing a game where you flip a coin with success probability $p$.
- If it comes up heads, you win $X$. If it comes up tails, you lose $Y$.
- What should we set $X$ and $Y$ for the game to be fair?

    $$E[earnings]= X p - Y (1 - p) = 0$$
- Implies
    $$\frac{Y}{X} = \frac{p}{1 - p}$$    
- The odds can be said as "How much should you be willing to pay for a $p$ probability of winning a dollar?"
    - (If $p > 0.5$ you have to pay more if you lose than you get if you win.)
    - (If $p < 0.5$ you have to pay less if you lose than you get if you win.)
    
For the logistic regression, 
$$p_x = \frac{e^{\beta_0 +\beta_1X}}{1 + e^{\beta_0 +\beta_1X}}$$

<<manupulate>>=
x <- seq(-10, 10, length = 1000)
beta1 <- 1
beta0 <- 1
plot(x, exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x)), 
         type = "l", lwd = 3, frame = FALSE)
@
As the $\beta_1$ shifts value, the curve will shift from being downward sloping (negative) to being upwards sloping (positive).  There is a straight line at zero.  $\beta_0$ will shift the whole line left or right. 

It is determining whether the data are zero or one. 

<<Reg>>=
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
summary(logRegRavens)
@

Given the following results from the logistic regression with Raven data
intercept coefficent -1.6 and slope 0.1006, this is saying that there is a log odds of -1.6 that they win when they have no points.  $e^{-1.6}$ would give the straight odds. $e^{0.1066}$ gives the increase in the odds for each point scored. 

For small numbers, $e^x \approx 1 + x$. The probability of winning is $\frac{e^{\beta_0 +\beta_1X}}{1 + e^{\beta_0 +\beta_1X}}$

Confidence intervals and Anova can be carried out. This will test whether adding variables reduces the variance by a significant amount. 

<<COnf>>=
exp(logRegRavens$coeff)
exp(confint(logRegRavens))
@

<<Anova>>>=
anova(logRegRavens,test="Chisq")
@
The residual deviance falls from 24.4 to 20.9 as the Raven score is added.  This is 3.54 increase, something that the Chi-squared distribution says would be seen only 6\% of the time if it were actually having no effect. 

\subsubsection{Poission Regression}
These are data that take the form of count. It could be the number of calls received by a call centre or rate, such as the percentage of children passing a test. The rate is a count per unit of time. In these cases, linear regression with a transformation is an option. The \emph{Poisson} distribution can also be used to model approximately binominal data with a small p and a large n or to model conteingency tables. 

\begin{equation}
X \sim Poission(t\lambda) 
\end{equation}

if 

\begin{equation}
P(X = x) = \frac{t\lambda^x e^{-t\lambda}}{x!}
\end{equation}

For $x = 1, 0 \dots $
The mean of the Poisson is $E[X] = t\lambda$.  Therefore, the $E[X/t] = \lambda$.  Therefore, $\lambda$ is the expected counts per unit of time. The variance of the Poisson is $t\lambda$.  The Poisson tends to normal as the $t\lambda$ gets large. 

<<plotPois, fig.height=4, cache=TRUE>>=
par(mfrow = c(1, 3))
plot(0 : 10, dpois(0 : 10, lambda = 2), type = "h", frame = FALSE)
plot(0 : 20, dpois(0 : 20, lambda = 10), type = "h", frame = FALSE)
plot(0 : 200, dpois(0 : 200, lambda = 100), type = "h", frame = FALSE)
@
Example, 

This example will work on hits to a web site per day. 
<<plotweb, fig.height=4>>=
load("../Data/gaData.rda")
head(gaData)
gaData$julian <- julian(gaData$date)
plot(gaData$julian,gaData$visits,pch=19,
     col="darkgrey",xlab="Julian",ylab="Visits")
@
It is possible to model this as a linear regression. For example,
\begin{equation}
NH_i = b_0 + b_1 JD_i + e_i
\end{equation}

where, 
\begin{itemize}
\item $NH_i$ is the number of hits to the web site
\item $JD_i$ is the day of the year (Julien)
\item $b_0$ is the number of hits on Julien day zero
\item $b_1$ is the increase in the number of hits per day. 
\item $e_I$ is the variation due to everything that is not measured. 
\end{itemize}

<<PlotReg, fig.height=4>>=
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
lm1 <- lm(gaData$visits ~ gaData$julian)
abline(lm1,col="red",lwd=3)
@
This whole exercise can be completed by logging the hits that are achieved (leaving aside for now that the log of the zero initial days is not possible).  However, this is a very special transformation that is being carried out. If that is the case, $e^{E[log(Y)]}$ is the geometric mean of Y.  With no covariates, this is estimated as $e^{\frac{1}{n}\sum_{t=1}^n log(y_t)}$.  This is the same as $(\prod_{t = 1}^n y_i)^{1/n}$.  The exponential coefficients estimate things about gemmetric means.
\begin{itemize}
\item $e^{\beta_0}$ is the geometric mean of the hits on day zero. 
\item $e^{\beta_1}$ is the geometric mean of the increase in hits per day.
\end{itemize}

There's a problem with logs with you have zero counts, adding a constant works, though this changes the interpretation as it is not the increase in the hits per day plus one. 
<<log>>=
round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 5)
@
This is not teling us that there is a 2\% increase in web hits per day.

The difference between Linear and Poisson regression is mainly in the interpretation of the coefficients. 

Linear vs. Poisson regression

The linear regression
$$ NH_i = b_0 + b_1 JD_i + e_i $$

implied an expected value relationship. 

$$ E[NH_i | JD_i, b_0, b_1] = b_0 + b_1 JD_i$$

The \emph{Poisson/log-linear} model will model the log of the expected value. 

$$ \log\left(E[NH_i | JD_i, b_0, b_1]\right) = b_0 + b_1 JD_i $$

Otherwise, this can be converted so that you get the expected value of the 

$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) $$

From this equation, $b_1$ may be interpreted as the relative increase in the hits per day holding everything else constant. 

$$ e^{(ENH_i | JD_i = J_1) - (E[NH_i|JD_i = j])} $$. 

This is different from the log of the count data because that would be $E[log(NH_i)]$




Multiplicative differences

$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) $$


$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 \right)\exp\left(b_1 JD_i\right) $$


If $JD_i$ is increased by one unit, $E[NH_i | JD_i, b_0, b_1]$ is multiplied by $\exp\left(b_1\right)$

<<poisReg, fig.height=4.5, cache=TRUE>>=
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
glm1 <- glm(gaData$visits ~ gaData$julian,family="poisson")
abline(lm1,col="red",lwd=3); lines(gaData$julian,glm1$fitted,col="blue",lwd=3)
@
Notice that the GLM model has a small curve. 

It appears that there are some issues with the residuals.  

<<resid2>>=
plot(glm1$fitted,glm1$residuals,pch=19,col="grey",ylab="Residuals",xlab="Fitted")
@
One way to deal with that would be to use the \emph{quasi-poisson} family argument. An alternative would be to try to get \emph{robust standard errors}.  

Fitting rates in R
Rate models.  To create a model of the rate, the \lstinline{offset} argument can be used.  This will define the variable that is to form the base of the rate. It must be logged.  See the slides for more details. 

<<rates, fig.height=4.5>>=
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
plot(julian(gaData$date),glm2$fitted,col="blue",pch=19,xlab="Date",ylab="Fitted Counts")
points(julian(gaData$date),glm1$fitted,col="red",pch=19)
@
<<rates2, fig.height=4.5>>=
## Fitting rates in R 
lm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col="grey",xlab="Date",
     ylab="Fitted Rates",pch=19)
lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col="blue",lwd=3)
@
ZIP model when there is inflation of the zero value that must be addressed. 

\end{document}